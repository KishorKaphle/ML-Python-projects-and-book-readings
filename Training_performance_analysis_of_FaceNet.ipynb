{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dXECVsnTY9zvoKEmkYTsjxFkjKDfa0vC",
      "authorship_tag": "ABX9TyOdr7Z4GPvdAwGo/2C1SLJr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KishorKaphle/ML-Python-projects-and-book-readings/blob/main/Training_performance_analysis_of_FaceNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfS4emoipU8f"
      },
      "source": [
        "**Using SVM classification method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXUnibnsm3KJ",
        "outputId": "31a6c1ee-c13c-43fe-ddc4-d421a52d5918",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from numpy import load\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.svm import SVC\n",
        "import time\n",
        "import numpy\n",
        "import pickle\n",
        "\n",
        "start_time = time.time()\n",
        "# load dataset\n",
        "data = load('/content/drive/My Drive/5-celebrity-faces-embeddings.npz')\n",
        "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "print('Dataset: train=%d, test=%d' % (trainX.shape[0], testX.shape[0]))\n",
        "# normalize input vectors\n",
        "in_encoder = Normalizer(norm='l2')\n",
        "trainX = in_encoder.transform(trainX)\n",
        "testX = in_encoder.transform(testX)\n",
        "# label encode targets\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(trainy)\n",
        "trainy = out_encoder.transform(trainy)\n",
        "testy = out_encoder.transform(testy)\n",
        "# fit model\n",
        "model = SVC(kernel='linear', probability=True)\n",
        "model.fit(trainX, trainy)\n",
        "\n",
        "numpy.save(('/content/drive/MyDrive/Xtrain'), trainX)\n",
        "numpy.save(('/content/drive/MyDrive/ytrain'), trainy)\n",
        "\n",
        "\n",
        "# predict\n",
        "yhat_train = model.predict(trainX)\n",
        "yhat_test = model.predict(testX)\n",
        "# score\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "# summarize\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f'\\nTime taken: {(end_time - start_time)*1000} miliseconds')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: train=93, test=25\n",
            "Accuracy: train=100.000, test=100.000\n",
            "\n",
            "Time taken: 32.21416473388672 miliseconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LeEwT46pLHp"
      },
      "source": [
        "**Using kNN clustering method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaaUqQBUn1ZQ",
        "outputId": "26653a5b-bda4-43f1-e245-81976980c0aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from numpy import load\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "# load dataset\n",
        "data = load('/content/drive/My Drive/5-celebrity-faces-embeddings.npz')\n",
        "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "print('Dataset: train=%d, test=%d' % (trainX.shape[0], testX.shape[0]))\n",
        "\n",
        "# normalize input vectors\n",
        "in_encoder = Normalizer(norm='l2')\n",
        "trainX = in_encoder.transform(trainX)\n",
        "testX = in_encoder.transform(testX)\n",
        "\n",
        "# label encode targets\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(trainy)\n",
        "trainy = out_encoder.transform(trainy)\n",
        "testy = out_encoder.transform(testy)\n",
        "\n",
        "# fit model\n",
        "model = KNeighborsClassifier(n_neighbors = 13)\n",
        "model.fit(trainX, trainy)\n",
        "\n",
        "# predict\n",
        "yhat_train = model.predict(trainX)\n",
        "yhat_test = model.predict(testX)\n",
        "\n",
        "# score\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "\n",
        "# summarize\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f'\\nTime taken: {(end_time - start_time)*1000} miliseconds')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: train=93, test=25\n",
            "Accuracy: train=100.000, test=100.000\n",
            "\n",
            "Time taken: 23.27251434326172 miliseconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54hyDN-lvPb2"
      },
      "source": [
        "**By using Random Forest Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDO1_qabvnBZ",
        "outputId": "828e40ce-0773-4552-c345-70ea850ab084",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from numpy import load\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "# load dataset\n",
        "data = load('/content/drive/My Drive/5-celebrity-faces-embeddings.npz')\n",
        "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "print('Dataset: train=%d, test=%d' % (trainX.shape[0], testX.shape[0]))\n",
        "\n",
        "# normalize input vectors\n",
        "in_encoder = Normalizer(norm='l2')\n",
        "trainX = in_encoder.transform(trainX)\n",
        "testX = in_encoder.transform(testX)\n",
        "\n",
        "# label encode targets\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(trainy)\n",
        "trainy = out_encoder.transform(trainy)\n",
        "testy = out_encoder.transform(testy)\n",
        "\n",
        "# fit model\n",
        "model = RandomForestClassifier(n_estimators=100)\n",
        "model.fit(trainX, trainy)\n",
        "\n",
        "# predict\n",
        "yhat_train = model.predict(trainX)\n",
        "yhat_test = model.predict(testX)\n",
        "\n",
        "# score\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "\n",
        "# summarize\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f'\\nTime taken: {(end_time - start_time)*1000} miliseconds')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: train=93, test=25\n",
            "Accuracy: train=100.000, test=100.000\n",
            "\n",
            "Time taken: 155.07149696350098 miliseconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkEy1yqAwV1D"
      },
      "source": [
        "**Naive Bayes Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WB7Z3LYw0Ek",
        "outputId": "5c9d3ab8-1ca7-440e-aec9-155c0d5de99d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from numpy import load\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "# load dataset\n",
        "data = load('/content/drive/My Drive/5-celebrity-faces-embeddings.npz')\n",
        "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "print('Dataset: train=%d, test=%d' % (trainX.shape[0], testX.shape[0]))\n",
        "\n",
        "# normalize input vectors\n",
        "in_encoder = Normalizer(norm='l2')\n",
        "trainX = in_encoder.transform(trainX)\n",
        "testX = in_encoder.transform(testX)\n",
        "\n",
        "# label encode targets\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(trainy)\n",
        "trainy = out_encoder.transform(trainy)\n",
        "testy = out_encoder.transform(testy)\n",
        "\n",
        "# fit model\n",
        "model = GaussianNB()\n",
        "model.fit(trainX, trainy)\n",
        "\n",
        "# predict\n",
        "yhat_train = model.predict(trainX)\n",
        "yhat_test = model.predict(testX)\n",
        "\n",
        "# score\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "\n",
        "# summarize\n",
        "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f'\\nTime taken: {(end_time - start_time)*1000} miliseconds')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: train=93, test=25\n",
            "Accuracy: train=100.000, test=100.000\n",
            "\n",
            "Time taken: 9.120941162109375 miliseconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKYYS8nUxLrp"
      },
      "source": [
        "http://www.isaet.org/images/extraimages/P1216004.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPq_l-Pw01eK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}